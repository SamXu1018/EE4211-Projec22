{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading data from July to September 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(month, days=31):\n",
    "    full_df = pd.read_csv(f\"./data/2022-{str(month)}-1.csv\")\n",
    "    for date in range(2, days+1):\n",
    "        test_df = pd.read_csv(f'./data/2022-{str(month)}-{date}.csv')\n",
    "        full_df = pd.concat([full_df, test_df])\n",
    "    full_df['timestamp'] = pd.to_datetime(full_df['timestamp'])\n",
    "    full_df = full_df.set_index('timestamp')\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5785166, 5)\n",
      "(5781908, 5)\n",
      "(1410593, 5)\n"
     ]
    }
   ],
   "source": [
    "jul = load_data(7)\n",
    "aug = load_data(8)\n",
    "sep = load_data(9, days=30)\n",
    "full_data = pd.concat([jul, aug, sep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['available rate'] = full_data['lots_available'] / full_data['total_lots']\n",
    "grped_full = full_data.groupby(full_data.carpark_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each carpark, we use the first 70% as training data and last 30% as testing data. We will use previous 8*24 hours availability as features to predict the next 24 hours availability.\n",
    "We slice out the data for each carpark and resample them to fill in missing values. Then we prepare training and testing features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method to create inputs and outputs from a given dataset\n",
    "def prep_train_test(dataset, feature_len):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(feature_len, len(dataset)):\n",
    "        X.append(dataset['available rate'][i-feature_len:i].values)\n",
    "        Y.append(dataset['available rate'][i:i+24].values)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total ids: 1966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_b/lkm0k5zn1cq5vrps1mrqwcb00000gn/T/ipykernel_57433/1625863988.py:10: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  slice = slice.resample('1h').mean()\n",
      "/var/folders/_b/lkm0k5zn1cq5vrps1mrqwcb00000gn/T/ipykernel_57433/699642481.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  Y = np.array(Y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lossy data. Dropped\n",
      "100 done\n",
      "200 done\n",
      "300 done\n",
      "400 done\n",
      "500 done\n",
      "Lossy data. Dropped\n",
      "Lossy data. Dropped\n",
      "600 done\n",
      "700 done\n",
      "800 done\n",
      "900 done\n",
      "1000 done\n",
      "1100 done\n",
      "Lossy data. Dropped\n",
      "1200 done\n",
      "1300 done\n",
      "1400 done\n",
      "1500 done\n",
      "1600 done\n",
      "1700 done\n",
      "1800 done\n",
      "1900 done\n"
     ]
    }
   ],
   "source": [
    "carparks = grped_full.carpark_number.unique()\n",
    "trainX, trainY = [], []\n",
    "testX, testY = [], []\n",
    "feature_len = 24*8\n",
    "split = 0.3\n",
    "print(\"total ids:\", len(carparks))\n",
    "index = 0\n",
    "for id in carparks:\n",
    "    slice = grped_full.get_group(id[0])\n",
    "    slice = slice.resample('1h').mean()\n",
    "    if slice.shape[0] != 2208:\n",
    "        print(\"Lossy data. Dropped\")\n",
    "        continue\n",
    "    split_index = int(slice.shape[0] * split)\n",
    "    X_train1, Y_train1 = prep_train_test(slice[:split_index], feature_len)\n",
    "    X_test1, Y_test1 = prep_train_test(slice[split_index:], feature_len)\n",
    "    trainX.extend(X_train1)\n",
    "    trainY.extend(Y_train1)\n",
    "    testX.extend(X_test1)\n",
    "    testY.extend(Y_test1)\n",
    "    index += 1\n",
    "    if index % 100 == 0:\n",
    "        print(index, \"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train test length: 922140 922140\n",
      "input shape: (192,)\n",
      "output shape: (24,)\n"
     ]
    }
   ],
   "source": [
    "print(\"train test length:\", len(trainX), len(trainY))\n",
    "print(\"input shape:\", trainX[0].shape)\n",
    "print(\"output shape:\", trainY[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: XXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have saved the most recent 8 days data in csv files. We can use the following code to generate a condensed csv file for recent data. The data in the condensed csv file will be used for prediction. Code below shows how to generate the condensed csv file from recent 8 days data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_b/lkm0k5zn1cq5vrps1mrqwcb00000gn/T/ipykernel_57433/1576522359.py:10: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  slice = slice.resample('1h').mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lossy data. Dropped\n",
      "Y49H\n"
     ]
    }
   ],
   "source": [
    "def generate_recent_csv():\n",
    "    df = load_data(11, days=8)\n",
    "    df['available rate'] = df['lots_available'] / df['total_lots']\n",
    "    grped = df.groupby(df.carpark_number)\n",
    "    carparks = grped.carpark_number.unique()\n",
    "    carparks = [carpark[0] for carpark in carparks]\n",
    "    new_df = pd.DataFrame(columns=carparks)\n",
    "    for id in carparks:\n",
    "        slice = grped.get_group(id)\n",
    "        slice = slice.resample('1h').mean()\n",
    "        if slice.shape[0] != 24*8:\n",
    "            print(\"Lossy data. Dropped\")\n",
    "            print(id)\n",
    "            continue\n",
    "        new_df[id] = slice['available rate'].values\n",
    "    new_df.index = slice.index\n",
    "    new_df.to_csv(\"./data/recent.csv\")\n",
    "\n",
    "generate_recent_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import PriorityQueue\n",
    "\n",
    "def find_nearest_x(x, y, location_dict):\n",
    "    shortest = PriorityQueue()\n",
    "    for key, loc in location_dict.items():\n",
    "        dist = (loc[0] - x)**2 + (loc[1] - y)**2\n",
    "        shortest.put((-dist, key))\n",
    "        if shortest.qsize() > 5:\n",
    "            shortest.get()\n",
    "    res = [shortest.get()[1] for i in range(5)]\n",
    "    res.reverse()\n",
    "    return res\n",
    "\n",
    "def create_location_dict(loc_info: pd.DataFrame):\n",
    "    location_dict = {}\n",
    "    for row in loc_info.iterrows():\n",
    "        content = row[1]\n",
    "        name = content[0]\n",
    "        location = (content[2], content[3])\n",
    "        location_dict[name] = location\n",
    "    return location_dict\n",
    "\n",
    "def model_predict(input):\n",
    "    return input[-24:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carpark ACB prediction: [0.64516129 0.64516129 0.64516129 0.55913978 0.5483871  0.52688172\n",
      " 0.43010753 0.05376344 0.08602151 0.         0.01075269 0.07526882\n",
      " 0.02150538 0.03225806 0.03225806 0.03225806 0.03225806 0.03225806\n",
      " 0.03225806 0.05376344 0.05376344        nan 0.03225806 0.03225806]\n",
      "carpark CY prediction: [0.35483871 0.35483871 0.35483871 0.16129032 0.09677419 0.16129032\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.                nan 0.         0.        ]\n",
      "carpark WCB prediction: [0.37062937 0.42657343 0.40559441 0.3986014  0.39160839 0.37762238\n",
      " 0.37762238 0.29370629 0.3006993  0.13986014 0.00699301 0.00699301\n",
      " 0.00699301 0.00699301 0.         0.         0.         0.\n",
      " 0.         0.00699301 0.00699301        nan 0.00699301 0.00699301]\n",
      "one of the nearest carpark SR2 is not in the recent data\n",
      "one of the nearest carpark SR1 is not in the recent data\n"
     ]
    }
   ],
   "source": [
    "loc_info = pd.read_csv(\"./data/hdb-carpark-information.csv\")\n",
    "recent = pd.read_csv(\"./data/recent.csv\")\n",
    "carpark_info = create_location_dict(loc_info)\n",
    "top5 = find_nearest_x(30314.7936, 31490.4942, carpark_info)\n",
    "for id in top5:\n",
    "    if id not in recent.columns:\n",
    "        print(f\"one of the nearest carpark {id} is not in the recent data\")\n",
    "        continue\n",
    "    col = recent[id][-24*8:]\n",
    "    prediction = model_predict(col)\n",
    "    print(f\"carpark {id} prediction: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('apple_tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06fa57ce78302be75248b003fd848fb7a1ff049a4cd6b32b7f26c1511486e4d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
